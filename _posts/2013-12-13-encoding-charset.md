---
layout: post
title: 前端工程师的编码问题
published: true
tags: [encoding charset unicode ucs UTF-8 UTF-16 编码 字符集]
---

## 基本概念

这篇文章分两个部分。第一部分梳理一下字符集和编码的基础知识，比如unicode、ucs、utf-8、utf-16等的关系，
第二部分整理一下前端工程师可能会遇到的一些编码问题。我的初衷是想能穷尽所有问题，但是我又知道这是不可能的，
所以只能尽量多的把搜集到的以及遇到过的问题整理出来。

### 字符集与编码

通常我们讲到的字符是一种文化符号，比如'中华'二字。顾名思义，字符集就是一套字符的集合。

众所周知，计算机只能处理/存储二进制数据，也就是一系列的0、1串。那么怎么用计算机来显示和处理人类文明使用的字符呢？
这就需要对字符进行编码，简单来说就是建立二进制数字和字符之间的对应关系。比如在
ASCII这套字符集中65就代表大写字母'A'。

那么，在计算机的范畴内, 字符集编码通常包含了两部分的内容：1、一套字符集
2、这套字符集与二进制数字的对应关系，这套对应关系叫做码表。比如繁体字'華'就不会在GB2312的码表中，
因为设计GB2312时就没有把繁体'華'字收录进去。

前面说到了字符集和码表的概念。现在说说字符怎么存储与读取。
为方便说明，后文主要以文本文件存储到硬盘和从硬盘中读取文件并显示为例。出于节省存储空间等方面的考虑，
通常字符并不是按照码表中对应的二进制直接存储的，而是将这个二进制数字再经过一次运算得到另一个更短的二进制串，
最终存储的是这个更短的二进制串，这个运算叫做编码。这有点类似twitter和微博等使用的短域名。

文本文件的大致存储过程是这样的：

1. 查找码表，得到字符对应的二进制串A
2. 将A按一定规则进行转换得到二进制串B
3. 将二进制B存储到硬盘

注意：步骤2可能没有。

读取文本文件就是相反的过程：

1. 从硬盘中读取数据，得到二进制串B
2. 将B按一定规则进行转换得到二进制串A
3. 查找码表，找到二进制串A对应的字符，并显示字符

注意：步骤2可能没有。

这个世界上存在着很多不同的字符集和各自相应的编码方式，比如ASCII，比如GB2312,GBK,
GB18030，同一套字符集可能会有不同的编码方式，比如UTF-8,UTF-16都使用unicode这套字符集。

当然你也可以设计自己的字符集，比如你设计一套字符集叫做前端字符集，
这套字符集只包括“前端工程师”这五个简体中文字，分别使用'0000000','0000001','0000010',
'0000011','0000100'，当读取数据时每次取一个字节，然后在码表中找到对应的字符，
如果读取的字节不是这5个字节的任意一个，那么你就显示为一个乱码。

好了，介绍了字符、字符集、码表、编码的基本概念，下面来看看中文程序员用到最多的几种编码。
(据我观察在程序员的日常交流中，很多时候字符集、字符集编码、编码泛指一个概念)

### ASCII

最初，计算机只在美国使用，由于英文符号比较少，美国人就使用1个字节来对他们使用到的字符进行编码，
这就是ASCII码(American Standard Code for Information Interchange，美国信息交换标准代码)。

1个字节可以表示256种不同的状态(2^8=256)，也就是说ASCII最多包含256个字符。实际上ASCII才使用到了第127号。
其中从0开始的32种状态被用做一些特殊用途。比如终端遇到0x07就会发出叫声。这些0x20以下的状态被成为'控制码'。

### ISO-8859-1

> ISO 8859-1，正式编号为ISO/IEC 8859-1:1998，又称Latin-1或“西欧语言”，
> 是国际标准化组织内ISO/IEC 8859的第一个8位字符集。它以ASCII为基础，在空置的0xA0-0xFF的范围内，
> 加入96个字母及符号，藉以供使用附加符号的拉丁字母语言使用。

它是ASCII的一种扩展字符集。其实后面要讲到的GB2312,GBK,GB18030都是对ASCII的扩展。

### GB2312

等到中国人使用计算机时发现ASCII里没有中文字符，而且一个字节最多编码256个字符，
远远不能满足中文的需要，中文的常用汉字就有6000多个。于是聪明的国人干脆就用两个字节来表示汉字。
规定：   

1. 一个小于127的字节与ASCII中对应的字符相同
2. 两个连续大于127的字节一起表示一个中文字符
3. 前面的一个字节（高字节）从0xA1用到0xF7，后面一个字节（低字节）从0xA1到0xFE，这样就可以组合出大约7000多个汉字了。

这套字符集和编码方案就是GB2312。

当读取数据时，先取一个字节A，如果A小于127，A就与其在ASCII中表示相同的含义(控制码或者字符);
如果A大于127，那么再取下一个字节B，如果B也大于127，那么AB一起表示一个汉字，在码表中查询其对应的字符。
如果码表中没有AB对应的字节，或者B小于127，那么就显示乱码或者进行出错处理。

### GBK和GB18030

后来人们发现GB2312的编码方式还是不够用，于是不再对低字节有要求，只要高字节大于127就表示连续两个字节表示一个汉字，
这样就能增加一些容量，可以加入新的字符了，于是就增加了近20000个新的汉字（包括繁体字）和符号，这就是GBK。

后来又加了几千个少数民族的字，这就是GB18030。

GB2312,GBK,GB18030这一系列字符集编码使用两个字节表示一个汉字，通称做"DBCS"(Double Byte Charecter Set 双字节字符集)。
这就是刚学程序设计时老师总要我们记住的'一个中文字符等于两个英文字符'。

### unicode和ISO-10646

世界上存在如此多的字符集编码，给交流造成了极大的困难。于是有人意识到有必要给全人类设计一套统一的字符集了。

不幸的是有两个独立的组织分别开始了这项工作，一个是国际标准化组织(ISO)，项目是ISO-10646;
另一个是由Apple，Xerox等公司于1988年组成的统一码盟(unicode.org)，项目就是unicode。

幸运的是，不久他们就发现了彼此的存在，两个项目的参与者都认识到，世界不需要两个不兼容的字符集。
于是它们开始合并双方的工作成果，并为创立一个单一编码表而协同工作。
1991年，不包含CJK统一汉字集的Unicode 1.0发布。随后，CJK统一汉字集的制定于1993年完成，
发布了ISO 10646-1:1993，即Unicode 1.1。

> 两个项目仍都独立存在，并独立地公布各自的标准。但统一码联盟和ISO都同意保持两者标准的码表兼容，
> 并紧密地共同调整任何未来的扩展。

也就是说，世界上存在两个统一字符集标准，但这两个标准是高度同步的，码表是一致的。
日常交流和技术书籍中使用unicode这个名字多一些。

unicode和ISO-10646定义的字符集就叫做通用字符集(Universal Character Set，UCS)。

unicode 1.x版本使用2个字节(16个位)编码，，这就是*UCS-2*。
从1996年发布unicode 2.0开始使用4个字节编码，这就是*UCS-4*。

UCS-2最多能表示2^16=65536个字符。UCS-4中只使用了31位，最高位为0。UCS-4在已使用的31位中，
按照最高字节可以分为2^7=128个group，每个group根据次高字节分为256个panel，
每个panel根据第三高字节分为256个row，每行有256个cells。
第0号group和第0号panel被称作**BMP**(Basic Multilingual Plane)。很明显BMP去掉高位的两个零字节就得到UCS-2。

在码表上每个字符对应的值称作code point。

### UTF-8、UTF-16、UTF-32和BOM

在介绍这部分之前，先简单介绍一下IETF。IETF(Internet Engineering Task Force)中文名是互联网工程任务组，
负责互联网标准的开发和推动。当前绝大多数国际互联网技术标准出自IETF，比如著名的http协议，代号是RFC2616。
比如前端工程师都熟悉的JSON格式也纳入到了IETF的维护, 代号是RFC4627。

#### UTF-8

UTF-8的全称是"8-bit  Unicode Transformation Format"。由IETF维护，代号是[RFC3629](http://www.ietf.org/rfc/rfc3629.txt)。
UTF-8是unicode(ISO-10646)的一种存储/传输方式。还记得前文中说到的文本文件存储的大致过程吗，
简单理解UTF-8就是第二步中的转换算法，由二进制串A(unicode code point)经过计算(UTF-8)得到二进制串B。

UTF-8的编码规则很简单，参照如下对照表。(表一)
<pre>
Char. number range     |        UTF-8 octet sequence
      (hexadecimal)    |              (binary)
   --------------------+-----------------------------------------
   0000 0000-0000 007F | 0xxxxxxx
   0000 0080-0000 07FF | 110xxxxx 10xxxxxx
   0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx
   0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
</pre>

以简体中文'中'字为例，按照RFC3629的描述，编码过程是：

1. 通过查表得到'中'的code point是4e2d，二进制形式是'100111000101101'
2. 对照表一第一列，'中'在第三行，对应的使用的UTF-8形式是1110xxxx 10xxxxxx 10xxxxxx
3. 从低位到高位，使用'100111000101101'填充1110xxxx 10xxxxxx 10xxxxxx中的x，未被填充的用0填充，最后得到'111001001011100010101100'

另一个简单的规律就是：

1. 对于单字节的符号，字节的第一位设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的。
2. 对于n字节的符号（n>1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。
剩下的未使用的二进制位，从低位到高位填充这个符号的unicode码。未填充的用0补齐。

解码就是相反的过程：

1. 读取第一个字节'11100100'，开头3个1说明一共是三个字节，然后再读取两个字节'10111000', '10101100'
2. 按照规则提取第一个字节除去开头'1110'的部分，得到'0100';第二、三个字节出去开头'10'的部分，'111000', '101100'
3. 将这个三部分拼接到一块得到'0100111000101100'，查表得到是'中'。

编码也会遇到撞码的情况：当我们以一种编码方式A存储数据得到二进制格式A1，而以另一种编码方式B读取数据A1，
恰好A1也符合B的编码方式。还记得著名的'联通'烧焦电池的问题吧：在window下新建文本文件，然后输入'联通'二字保存，
再用记事本程序打开，'联通'二字不见了，取而代之的是一个烧焦电池的图形。让我们来分析一下这个问题，

* 首先，新建文本文件时默认的格式ANSI，对于英文文件是ASCII编码，
对于简体中文文件是GB系列编码(只针对Windows简体中文版，如果是繁体中文版会采用Big5码)
* 输入'联通'后保存文件是GB系列编码的，以十六进制查看就是<code>c1 aa cd a8</code>，对应的二进制是

        parseInt( 'c1aacda8', 16 ).toString( 2 ) // 11000001 10101010 11001101 10101000
* 当再次打开文件时，记事本程序首选会尝试用UTF-8的格式解析数据，恰巧这4个字节符合UTF-8的编码规则，
前两个字节转换得到1101010，对应的字符是'j'，后两个字节转换得到1101101000，没有这个code point，所以显示为乱码。
* 当打开文件时，文本编辑器都会尝试用不同的编码格式读取数据，当首选的编码格式读取有错误就会尝试使用其他的格式，
比如vim都会设置

        set fencs=utf-8,gb18030,gbk,gb2312,cp936,ucs-bom,euc-jp,
当尝试使用utf-8失败时就会使用gb18030，以此类推。如果将utf-8设置在gb2312之后，就能"正常打开"。
同样这也解释了多保存'移动'两个字后就不会出现乱码，因为移动按GB系编码后不会与utf-8撞码，
当试图使用utf-8读取时会发生错误，文本编辑器就会尝试使用其他的编码方式读取。




